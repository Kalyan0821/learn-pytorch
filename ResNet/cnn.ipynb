{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop((32, 32), padding=4, padding_mode=\"reflect\"),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010], inplace=True)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010], inplace=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR10(\"../5. CNN/cifar-10\", download=True, train=True, transform=train_transform)\n",
    "test_dataset = CIFAR10(\"../5. CNN/cifar-10\", train=False, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=400, shuffle=True, num_workers=12, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=800, num_workers=6, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet9(nn.Module):\n",
    "    \n",
    "    def __init__(self, c=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = self.conv_layer(c, 64)\n",
    "        self.conv2 = self.conv_layer(64, 128, pool=True)\n",
    "        self.conv3_res1 = self.conv_layer(128, 128)\n",
    "        self.conv4_res1 = self.conv_layer(128, 128, activate=False)\n",
    "        self.conv5 = self.conv_layer(128, 256, pool=True)\n",
    "        self.conv6 = self.conv_layer(256, 512, pool=True)\n",
    "        self.conv7_res2 = self.conv_layer(512, 512)\n",
    "        self.conv8_res2 = self.conv_layer(512, 512, activate=False)\n",
    "        self.pool8 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.classifier9 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=512, out_features=10)\n",
    "        )\n",
    "    \n",
    "    def conv_layer(self, c_in, c_out, activate=True, pool=False):\n",
    "        layers = [\n",
    "                nn.Conv2d(in_channels=c_in, out_channels=c_out, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(num_features=c_out) \n",
    "        ]\n",
    "        if activate:\n",
    "            layers.append(nn.ReLU())\n",
    "        else:\n",
    "            assert pool is False\n",
    "            \n",
    "        if pool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        a1 = self.conv1(xb)\n",
    "        a2 = self.conv2(a1)\n",
    "        a3 = self.conv3_res1(a2)\n",
    "        \n",
    "        z4 = self.conv4_res1(a3)\n",
    "        a4 = nn.ReLU()(z4 + a2)\n",
    "        \n",
    "        a5 = self.conv5(a4)\n",
    "        a6 = self.conv6(a5)\n",
    "        a7 = self.conv7_res2(a6)\n",
    "        \n",
    "        z8 = self.conv8_res2(a7)\n",
    "        a8 = nn.ReLU()(z8 + a6)\n",
    "        a8 = self.pool8(a8)\n",
    "        \n",
    "        out = self.classifier9(a8)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, alpha, num_epochs, weight_decay=0, grad_clip=None):\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=alpha, weight_decay=weight_decay)\n",
    "    # Learning rate scheduling\n",
    "    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=alpha, \n",
    "                                        epochs=num_epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        model.train()  # Set back to training mode\n",
    "        \n",
    "        batch_losses = []\n",
    "        for xb, yb in tqdm(train_loader):\n",
    "            xb, yb = xb.to(device), yb.to(device)  # Copies batch to gpu\n",
    "            \n",
    "            zb = model(xb)\n",
    "            loss = loss_function(zb, yb)\n",
    "            loss.backward()\n",
    "            if grad_clip:\n",
    "                clip_grad_value_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            last_lr = optimizer.param_groups[0][\"lr\"]  # Last learning rate used in the current epoch\n",
    "            scheduler.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "        \n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, loss_function)\n",
    "        print(f\"Epoch: {e+1}, Last lr: {last_lr}, Train Loss: {epoch_loss}, Val Loss: {val_loss}, Val Acc: {val_acc}\") \n",
    "        \n",
    "        \n",
    "def evaluate(model, val_loader, loss_function=None):\n",
    "    model.eval()  # Set to evaluation mode, so BatchNorm/Dropout will behave correctly \n",
    "    \n",
    "    with torch.no_grad():  # No computation graph    \n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_accs = []\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)  # Copies batch to gpu\n",
    "\n",
    "            zb = model(xb)\n",
    "            if loss_function is not None:\n",
    "                loss = loss_function(zb, yb)\n",
    "                batch_losses.append(loss.item())\n",
    "\n",
    "            batch_accs.append(accuracy(zb, yb))\n",
    "\n",
    "    if loss_function is None:\n",
    "        return np.mean(batch_accs)\n",
    "    else:\n",
    "        return [np.mean(batch_losses), np.mean(batch_accs)]\n",
    "    \n",
    "def accuracy(zb, yb):\n",
    "    _, predicted = torch.max(zb, dim=1)\n",
    "    acc = torch.sum(predicted==yb) / yb.shape[0]\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet9()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 0.01\n",
    "num_epochs = 8\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.33it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last lr: 0.003929274947543202 Train Loss: 1.4836134548187256, Val Loss: 1.358323574066162, Val Acc: 0.5743269140903766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.35it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Last lr: 0.009352712767003208 Train Loss: 1.0447519469261168, Val Loss: 1.1338582222278302, Val Acc: 0.6505769124397864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:22<00:00,  5.53it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Last lr: 0.009719417773875232 Train Loss: 0.7896337532997131, Val Loss: 0.7748635136164151, Val Acc: 0.7301922944875864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.24it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Last lr: 0.008117456539497631 Train Loss: 0.6112672920227051, Val Loss: 0.6978815885690542, Val Acc: 0.7440384442989643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.32it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Last lr: 0.005559840141227017 Train Loss: 0.486379576921463, Val Loss: 0.48605384276463437, Val Acc: 0.8332692017922034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:22<00:00,  5.47it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Last lr: 0.0028306099820869924 Train Loss: 0.3876775047779083, Val Loss: 0.39436071194135225, Val Acc: 0.8651922941207886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:23<00:00,  5.41it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Last lr: 0.0007664159383425639 Train Loss: 0.28283466649055483, Val Loss: 0.31125691303840053, Val Acc: 0.8956730595001807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:22<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Last lr: 4e-08 Train Loss: 0.21663150990009308, Val Loss: 0.28656160143705517, Val Acc: 0.9048076776357797\n",
      "CPU times: user 3min 13s, sys: 4.41 s, total: 3min 18s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit(model, train_loader, test_loader, alpha=max_lr, num_epochs=num_epochs, weight_decay=weight_decay, grad_clip=grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9048076776357797\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9048076776357797"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"./saved_model.pth\")\n",
    "\n",
    "model2 = ResNet9().to(device)\n",
    "model2.load_state_dict(torch.load(\"./saved_model.pth\"))\n",
    "\n",
    "evaluate(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
